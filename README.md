# ü§ñ Production-Ready RAG Application

A complete Retrieval-Augmented Generation (RAG) application with advanced features including query expansion, reranking, and conversation memory. Built with LangChain, LangGraph, and Streamlit.

[![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=flat&logo=streamlit&logoColor=white)](https://streamlit.io)
[![LangChain](https://img.shields.io/badge/LangChain-121212?style=flat&logo=chainlink&logoColor=white)](https://langchain.com)
[![Pinecone](https://img.shields.io/badge/Pinecone-000000?style=flat&logo=pinecone&logoColor=white)](https://pinecone.io)

---

## üìã Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Project Structure](#-project-structure)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Running the Application](#-running-the-application)
- [Usage Guide](#-usage-guide)
- [Deployment to Streamlit Cloud](#-deployment-to-streamlit-cloud)
- [Advanced Features](#-advanced-features)
- [Troubleshooting](#-troubleshooting)

---

## ‚ú® Features

### Document Ingestion
- üìÑ **Multi-format Support**: PDF, DOCX, TXT, Markdown
- üî™ **Advanced Chunking Strategies**:
  - **Recursive**: Fast, fixed-size chunks with configurable size and overlap
  - **Semantic**: Context-aware splitting using embeddings
  - **Agentic**: AI-powered intelligent proposition extraction
- üóÑÔ∏è **Vector Storage**: Pinecone integration with automatic index management
- üìä **Progress Tracking**: Real-time logs and processing statistics

### RAG Query Pipeline
- üîç **Query Expansion**: Multi-query retrieval for better results
- üéØ **Smart Reranking**: Cohere API or HuggingFace cross-encoders
- ü§ñ **Query Classification**: Intelligent routing (greeting vs RAG query)
- üí¨ **Conversation Memory**: Persistent chat history with LangGraph
- üìö **Source Citations**: View retrieved document chunks with relevance scores
- üé® **Score Filtering**: Automatic filtering of low-quality sources (< 0.1 threshold)

### User Interface
- üéØ **Clean Design**: Modern Streamlit interface
- üìë **Two-Tab Layout**: Separate views for ingestion and chat
- üíæ **Session Management**: Persistent conversations
- üîÑ **Real-time Updates**: Live processing logs
- üì± **Responsive**: Works on desktop and mobile

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         STREAMLIT UI                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Document Ingestion Tab  ‚îÇ  ‚îÇ      Chat Tab            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Upload files          ‚îÇ  ‚îÇ  - Query documents       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Configure chunking    ‚îÇ  ‚îÇ  - View sources          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - View statistics       ‚îÇ  ‚îÇ  - Chat history          ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      BACKEND (main_app.py)                      ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Ingestion Pipeline ‚îÇ         ‚îÇ    RAG Pipeline          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                    ‚îÇ         ‚îÇ    (LangGraph)           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ 1. Parse docs      ‚îÇ         ‚îÇ                          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ 2. Chunk text      ‚îÇ         ‚îÇ  Classify ‚Üí Expand ‚Üí     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ 3. Embed chunks    ‚îÇ         ‚îÇ  Retrieve ‚Üí Rerank ‚Üí     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ 4. Store in DB     ‚îÇ         ‚îÇ  Generate                ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    EXTERNAL SERVICES                            ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  üìä Pinecone VectorDB  |  ü§ñ OpenAI/Gemini  |  üéØ Cohere      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### RAG Pipeline Flow (LangGraph)

```
START ‚Üí Classify Query
          ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚Üì           ‚Üì
Greeting?    RAG Query?
    ‚Üì           ‚Üì
Simple      Expand Query
Response        ‚Üì
    ‚Üì       Retrieve Docs (Multi-query)
    ‚Üì           ‚Üì
    ‚Üì       Rerank (Score > 0.1)
    ‚Üì           ‚Üì
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚Üí  Generate Answer
                ‚Üì
              END
```

---

## üìÇ Project Structure

```
RAG/
‚îú‚îÄ‚îÄ app.py                          # Entry point for Streamlit
‚îú‚îÄ‚îÄ config.py                       # Configuration (loads .env)
‚îú‚îÄ‚îÄ requirements.txt                # Python dependencies
‚îú‚îÄ‚îÄ README.md                       # This file
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main_app.py                # üî¥ BACKEND: Core RAG pipeline
‚îÇ   ‚îú‚îÄ‚îÄ rag_frontend.py            # üé® FRONTEND: Streamlit UI
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ ingestion/                 # Document processing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DocumentParsers.py    # PDF, DOCX, TXT, MD parsers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChunkCreator.py       # Recursive, Semantic, Agentic chunking
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ EmbeddingCreator.py   # OpenAI/HuggingFace embeddings
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ DBIngestion.py        # Pinecone vector store operations
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ generation/                # LLM generation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ llm_generator.py      # OpenAI/Gemini + query expansion
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ reranking/                 # Document reranking
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ reranker.py           # Cohere + Cross-encoder rerankers
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ retrieval/                 # (Future: advanced retrieval)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ utils/                     # Helper functions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Helpers.py            # Utility functions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PromptsConstants.py  # System prompts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ AgenticChunkerHelper.py  # Agentic chunking logic
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ ui/                        # UI components
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ components.py         # Reusable Streamlit components
‚îÇ
‚îî‚îÄ‚îÄ .env                           # Environment variables (create this)
```

---

## üöÄ Installation

### Prerequisites

- Python 3.9+
- pip or conda

### Step 1: Clone or Download

```bash
cd /path/to/RAG
```

### Step 2: Create Virtual Environment (Recommended)

```bash
# Using venv
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Using conda
conda create -n rag-app python=3.9
conda activate rag-app
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

---

## ‚öôÔ∏è Configuration

### Create `.env` File

Create a `.env` file in the project root:

```bash
# Required
OPENAI_API_KEY=sk-your-openai-api-key
PINECONE_API_KEY=your-pinecone-api-key
PINECONE_ENVIRONMENT=us-east-1-aws  # Your Pinecone environment

# Optional
COHERE_API_KEY=your-cohere-api-key        # For reranking (recommended)
GEMINI_API_KEY=your-gemini-api-key        # Alternative LLM provider
LANGCHAIN_API_KEY=your-langsmith-key      # For tracing/debugging
LANGCHAIN_TRACING_V2=true                 # Enable LangSmith tracing

# Provider Selection (defaults to OpenAI)
LLM_PROVIDER=openai                       # or "gemini"
EMBEDDING_PROVIDER=openai                 # or "huggingface"
```

### Environment Variables Explained

| Variable | Required | Description |
|----------|----------|-------------|
| `OPENAI_API_KEY` | ‚úÖ Yes | OpenAI API key for embeddings and LLM |
| `PINECONE_API_KEY` | ‚úÖ Yes | Pinecone API key for vector storage |
| `PINECONE_ENVIRONMENT` | ‚úÖ Yes | Your Pinecone environment region |
| `COHERE_API_KEY` | ‚ö†Ô∏è Recommended | Cohere API for better reranking |
| `GEMINI_API_KEY` | ‚ùå Optional | Use Google's Gemini instead of OpenAI |
| `LANGCHAIN_API_KEY` | ‚ùå Optional | Enable LangSmith tracing for debugging |

---

## üéÆ Running the Application

### Quick Start (Recommended)

```bash
streamlit run app.py
```

The app will open in your browser at `http://localhost:8501`

### Alternative Methods

```bash
# Method 1: From root directory
python -m streamlit run src/rag_frontend.py

# Method 2: Direct run
cd src
streamlit run rag_frontend.py
```

---

## üìñ Usage Guide

### 1Ô∏è‚É£ Document Ingestion

1. **Navigate to "Document Ingestion" tab** (default view)

2. **Configure Chunking Strategy**:
   - **Recursive**: Shows chunk size and overlap sliders (good for most cases)
   - **Semantic**: No configuration needed (context-aware, slower)
   - **Agentic**: No configuration needed (AI-powered, slowest but best quality)

3. **Upload Documents**:
   - Click "Browse files" or drag & drop
   - Supported formats: PDF, DOCX, TXT, MD
   - Multiple files supported

4. **Process Documents**:
   - Click "Process Documents"
   - Watch real-time logs in the expander
   - View results table with statistics

5. **View Statistics**:
   - Total files processed
   - Total chunks created
   - Success rate
   - Individual file results

### 2Ô∏è‚É£ Chat with Documents

1. **Navigate to "Chat" tab**

2. **Ask Questions**:
   - Type your question in the chat input
   - Press Enter or click send
   - Wait for AI response (streaming)

3. **View Sources**:
   - Click "View Sources" below each response
   - Expand individual sources to see document chunks
   - Check relevance scores

4. **Manage Conversations**:
   - **New Chat**: Start fresh conversation (sidebar)
   - **Select Thread**: Resume previous conversation
   - **Delete Thread**: Remove old conversations

### üí° Pro Tips

- **Query Classification**: The system automatically detects if your query needs document retrieval or is just a greeting/simple question
- **Source Filtering**: Only sources with score ‚â• 0.1 are shown (automatic quality filtering)
- **Conversation Context**: Each thread maintains its own conversation history
- **Score Interpretation**: Higher scores (closer to 1.0) indicate more relevant sources

---

## ‚òÅÔ∏è Deployment to Streamlit Cloud

### Pre-Deployment Checklist

‚úÖ All code is cloud-ready (no filesystem dependencies)
‚úÖ Pinecone stores all document data (persists across restarts)
‚úÖ Conversation history in SQLite (ephemeral but acceptable)

### Step 1: Prepare Repository

```bash
# Ensure requirements.txt is up to date
pip freeze > requirements.txt

# Create .streamlit/config.toml (optional)
mkdir -p .streamlit
cat > .streamlit/config.toml << EOF
[theme]
primaryColor = "#FF4B4B"
backgroundColor = "#FFFFFF"
secondaryBackgroundColor = "#F0F2F6"
textColor = "#262730"
font = "sans serif"
EOF
```

### Step 2: Push to GitHub

```bash
git init
git add .
git commit -m "Initial commit: RAG application"
git remote add origin <your-repo-url>
git push -u origin main
```

### Step 3: Deploy on Streamlit Cloud

1. Go to [share.streamlit.io](https://share.streamlit.io)
2. Click "New app"
3. Select your GitHub repository
4. Set **Main file path**: `app.py`
5. Set **Python version**: 3.9 or higher

### Step 4: Configure Secrets

In Streamlit Cloud dashboard, go to **App settings ‚Üí Secrets** and add:

```toml
OPENAI_API_KEY = "sk-your-openai-api-key"
PINECONE_API_KEY = "your-pinecone-api-key"
PINECONE_ENVIRONMENT = "us-east-1-aws"
COHERE_API_KEY = "your-cohere-api-key"
LANGCHAIN_API_KEY = "your-langsmith-key"
LANGCHAIN_TRACING_V2 = "true"
```

### Step 5: Deploy!

Click "Deploy" and wait for the app to build (2-3 minutes).

### ‚ö†Ô∏è Important Notes for Cloud Deployment

- **Conversation History**: Will be lost on app restart (ephemeral SQLite)
  - For production, consider using PostgreSQL via Streamlit's connection feature
- **Document Metadata**: Stored in Pinecone (persists across restarts) ‚úÖ
- **File Uploads**: Processed immediately, not stored locally ‚úÖ

---

## üéØ Advanced Features

### Query Expansion

The system automatically generates multiple query variants to improve retrieval:

```python
Original Query: "What is the loan interest rate?"

Expanded Queries:
1. "What is the loan interest rate?"
2. "What is the APR for the loan?"
3. "What are the loan terms and interest charges?"
```

### Reranking with Score Filtering

Documents are reranked using Cohere's `rerank-v3.5` model and filtered:

```python
Retrieved: 10 documents
Reranked:  10 documents
Filtered:  5 documents (score >= 0.1)
‚Üí Only high-quality sources shown to user
```

### Query Classification

Intelligent routing saves tokens and improves UX:

```
User: "Hi"
‚Üí Classification: greeting
‚Üí Simple response (no RAG)

User: "What is the refund policy?"
‚Üí Classification: rag_query
‚Üí Full RAG pipeline with document retrieval
```

### Agentic Chunking

AI-powered chunking extracts atomic propositions:

```
Original Text:
"John works at Microsoft. Microsoft is located in Seattle.
Seattle is in Washington state."

Propositions:
1. John works at Microsoft
2. Microsoft is located in Seattle
3. Seattle is in Washington state

‚Üí Each proposition becomes a semantically complete chunk
```

---

## üõ†Ô∏è Troubleshooting

### Common Issues

#### 1. `ModuleNotFoundError: No module named 'src'`

**Solution**: Always run from the root directory using `app.py`:
```bash
streamlit run app.py
```

#### 2. API Key Errors

**Symptoms**:
```
The api_key client option must be set...
```

**Solution**:
1. Check `.env` file exists in project root
2. Verify API keys are correct
3. Restart the application

#### 3. Pinecone Index Not Found

**Symptoms**:
```
Index 'rag-documents' does not exist
```

**Solution**: The index is created automatically on first document upload. Just upload documents via the Ingestion tab.

#### 4. LangChain Deprecation Warnings

**Symptoms**:
```
LangChainDeprecationWarning: ...
```

**Solution**: These are warnings, not errors. The code uses the latest LangChain APIs. Upgrade LangChain if needed:
```bash
pip install --upgrade langchain langchain-core langchain-openai
```

#### 5. Streamlit Duplicate Key Errors

**Fixed**: The app now generates unique message IDs to prevent key conflicts.

#### 6. Conversation History Not Loading

**Symptoms**: Previous conversations don't load

**Solution**:
- Check `chatbot.db` exists in project root
- On Streamlit Cloud, conversations reset on restart (this is expected)
- For persistent history, upgrade to PostgreSQL

### Debug Mode

Enable LangSmith tracing for detailed debugging:

```bash
# In .env
LANGCHAIN_API_KEY=your_langsmith_key
LANGCHAIN_TRACING_V2=true
```

Then view traces at [smith.langchain.com](https://smith.langchain.com)

---

## üìä Performance Tips

### For Faster Ingestion
- Use **Recursive chunking** (fastest)
- Upload files in batches of 5-10
- Optimize chunk size: 500-1000 characters works well

### For Better Retrieval
- Use **Semantic or Agentic chunking** (slower but better quality)
- Enable **Cohere reranking** (significant improvement)
- Set `top_k=10` for retrieval, `rerank_top_k=5` for final results

### For Lower Costs
- Use `gpt-3.5-turbo` instead of `gpt-4` (edit `config.py`)
- Reduce `top_k` to retrieve fewer documents
- Use Gemini as alternative (cheaper)

---

## ü§ù Contributing

Contributions are welcome! Areas for improvement:

- [ ] Add support for more document formats (HTML, CSV, Excel)
- [ ] Implement hybrid search (keyword + vector)
- [ ] Add document deletion feature
- [ ] Support for multiple Pinecone indexes
- [ ] Add authentication/user management
- [ ] Implement streaming for reranking
- [ ] Add evaluation metrics (RAGAS, etc.)

---

## üìù License

This project is provided as-is for educational and commercial use.

---

## üôè Acknowledgments

Built with:
- [LangChain](https://langchain.com) - LLM framework
- [LangGraph](https://langchain-ai.github.io/langgraph/) - Graph-based workflows
- [Streamlit](https://streamlit.io) - Web UI framework
- [Pinecone](https://pinecone.io) - Vector database
- [OpenAI](https://openai.com) - LLM and embeddings
- [Cohere](https://cohere.com) - Reranking

---

## üìß Support

For issues and questions:
- Check the [Troubleshooting](#-troubleshooting) section
- Review LangChain docs: [python.langchain.com](https://python.langchain.com)
- Review Streamlit docs: [docs.streamlit.io](https://docs.streamlit.io)

---
